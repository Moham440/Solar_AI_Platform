"""
Solar AI Platform - Plateforme Intelligente de Pr√©diction et Supervision des Centrales Solaires
Niveau : National - Industriel - ERA 2026 - Startup Ready

Auteur : Chalabi Mohammed El Amine
Version : 1.0.0
Date : Janvier 2026
"""

import os
import sys
import warnings
import logging
from pathlib import Path
from datetime import datetime, timedelta
import numpy as np
import pandas as pd
import pickle
from typing import Dict, List, Tuple, Optional

# Machine Learning
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Visualisation
import matplotlib.pyplot as plt
import seaborn as sns

# Configuration
warnings.filterwarnings('ignore')
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 6)

# Configuration des logs (sans emojis pour compatibilit√© Windows)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('solar_ai_platform.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class SolarAIPlatform:
    """
    Plateforme compl√®te de gestion intelligente des centrales solaires
    """

    def __init__(self, data_path: Optional[str] = None):
        """
        Initialisation de la plateforme Solar AI

        Args:
            data_path: Chemin vers le fichier de donn√©es CSV
        """
        self.base_dir = Path(__file__).parent
        self.data_dir = self.base_dir / "data"
        self.models_dir = self.base_dir / "models"
        self.outputs_dir = self.base_dir / "outputs"

        # Cr√©ation des dossiers
        for directory in [self.data_dir, self.models_dir, self.outputs_dir]:
            directory.mkdir(parents=True, exist_ok=True)

        # Variables de stockage
        self.df_raw = None
        self.df_processed = None
        self.df_train = None
        self.df_test = None
        self.models = {}
        self.scaler = StandardScaler()
        self.anomaly_detector = None
        self.feature_importance = {}

        # M√©triques
        self.metrics = {}
        self.predictions = {}
        self.anomalies = None

        logger.info("[OK] Solar AI Platform initialis√©e avec succ√®s")
        logger.info(f"üìÅ Dossier de travail : {self.base_dir}")

        if data_path:
            self.load_data(data_path)

    def load_data(self, filepath: str) -> pd.DataFrame:
        """
        Chargement et v√©rification des donn√©es

        Args:
            filepath: Chemin vers le fichier CSV

        Returns:
            DataFrame charg√©
        """
        try:
            logger.info(f"üìÇ Chargement des donn√©es depuis : {filepath}")

            # Lecture du fichier
            self.df_raw = pd.read_csv(filepath)
            logger.info(f"[OK] {len(self.df_raw)} lignes charg√©es")

            # V√©rifications
            logger.info("üîç V√©rification de la qualit√© des donn√©es...")

            # Valeurs manquantes
            missing = self.df_raw.isnull().sum()
            if missing.any():
                logger.warning(f"‚ö†Ô∏è Valeurs manquantes d√©tect√©es :\n{missing[missing > 0]}")

            # Doublons
            duplicates = self.df_raw.duplicated().sum()
            if duplicates > 0:
                logger.warning(f"‚ö†Ô∏è {duplicates} doublons d√©tect√©s")
                self.df_raw = self.df_raw.drop_duplicates()

            # Affichage des colonnes
            logger.info(f"[DATA] Colonnes disponibles : {list(self.df_raw.columns)}")
            logger.info(f"üìà Statistiques de base :\n{self.df_raw.describe()}")

            return self.df_raw

        except FileNotFoundError:
            logger.error(f"‚ùå Fichier non trouv√© : {filepath}")
            raise
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement : {str(e)}")
            raise

    def generate_sample_data(self, n_inverters: int = 5, days: int = 30) -> pd.DataFrame:
        """
        G√©n√©ration de donn√©es simul√©es r√©alistes pour d√©monstration

        Args:
            n_inverters: Nombre d'onduleurs
            days: Nombre de jours de donn√©es

        Returns:
            DataFrame avec donn√©es simul√©es
        """
        logger.info(f"[BUILD] G√©n√©ration de donn√©es simul√©es : {n_inverters} onduleurs, {days} jours")

        np.random.seed(42)

        # G√©n√©ration de timestamps
        start_date = datetime.now() - timedelta(days=days)
        timestamps = pd.date_range(start=start_date, periods=days*24*4, freq='15min')

        data = []

        for inv_id in range(1, n_inverters + 1):
            # Performance al√©atoire par onduleur (certains moins performants)
            efficiency_factor = np.random.uniform(0.85, 1.0)

            for ts in timestamps:
                hour = ts.hour
                month = ts.month

                # Irradiation solaire (W/m¬≤) - profil journalier r√©aliste
                if 6 <= hour <= 18:
                    base_irradiance = 1000 * np.sin((hour - 6) * np.pi / 12)
                    seasonal_factor = 1.0 + 0.3 * np.sin((month - 1) * np.pi / 6)
                    irradiance = base_irradiance * seasonal_factor * np.random.uniform(0.8, 1.1)
                else:
                    irradiance = 0

                # Temp√©rature ambiante (¬∞C)
                temp_ambient = 20 + 15 * np.sin((month - 1) * np.pi / 6) + \
                              10 * np.sin((hour - 12) * np.pi / 12) + \
                              np.random.normal(0, 2)

                # Temp√©rature du module (plus √©lev√©e que l'ambiante)
                temp_module = temp_ambient + irradiance * 0.025 + np.random.normal(0, 3)

                # Tension et courant DC
                dc_voltage = 600 + np.random.normal(0, 20)
                dc_current = irradiance * 0.01 * efficiency_factor + np.random.normal(0, 0.5)
                dc_power = dc_voltage * dc_current

                # Puissance AC (avec pertes de conversion)
                conversion_efficiency = 0.95 + np.random.normal(0, 0.02)
                ac_power = dc_power * conversion_efficiency

                # Injection d'anomalies al√©atoires (5% des cas)
                if np.random.random() < 0.05:
                    ac_power *= np.random.uniform(0.5, 0.8)  # Baisse de performance

                data.append({
                    'Timestamp': ts,
                    'Inverter_ID': f'INV_{inv_id:03d}',
                    'DC_Voltage': max(0, dc_voltage),
                    'DC_Current': max(0, dc_current),
                    'DC_Power': max(0, dc_power),
                    'AC_Power': max(0, ac_power),
                    'Ambient_Temperature': temp_ambient,
                    'Module_Temperature': temp_module,
                    'Irradiance': max(0, irradiance)
                })

        self.df_raw = pd.DataFrame(data)

        # Sauvegarde
        output_path = self.data_dir / "merged_cleaned_data.csv"
        self.df_raw.to_csv(output_path, index=False)
        logger.info(f"[OK] Donn√©es simul√©es g√©n√©r√©es : {len(self.df_raw)} lignes")
        logger.info(f"üíæ Sauvegard√© dans : {output_path}")

        return self.df_raw

    def feature_engineering(self) -> pd.DataFrame:
        """
        Ing√©nierie avanc√©e des caract√©ristiques

        Returns:
            DataFrame avec features enrichies
        """
        logger.info("[BUILD] D√©marrage de l'ing√©nierie des caract√©ristiques...")

        df = self.df_raw.copy()

        # Conversion du timestamp
        df['Timestamp'] = pd.to_datetime(df['Timestamp'])

        # === FEATURES TEMPORELLES ===
        logger.info("‚è∞ Cr√©ation des features temporelles...")
        df['Hour'] = df['Timestamp'].dt.hour
        df['Day'] = df['Timestamp'].dt.day
        df['Month'] = df['Timestamp'].dt.month
        df['DayOfWeek'] = df['Timestamp'].dt.dayofweek
        df['Quarter'] = df['Timestamp'].dt.quarter

        # Saisons
        df['Season'] = df['Month'].apply(lambda x:
            1 if x in [12, 1, 2] else
            2 if x in [3, 4, 5] else
            3 if x in [6, 7, 8] else 4
        )

        # === FEATURES CYCLIQUES ===
        logger.info("üîÑ Cr√©ation des features cycliques...")
        df['Hour_Sin'] = np.sin(2 * np.pi * df['Hour'] / 24)
        df['Hour_Cos'] = np.cos(2 * np.pi * df['Hour'] / 24)
        df['Month_Sin'] = np.sin(2 * np.pi * df['Month'] / 12)
        df['Month_Cos'] = np.cos(2 * np.pi * df['Month'] / 12)

        # === RATIOS ET EFFICACIT√â ===
        logger.info("‚ö° Calcul des ratios de performance...")
        df['DC_AC_Ratio'] = df['DC_Power'] / (df['AC_Power'] + 1e-6)
        df['Conversion_Efficiency'] = df['AC_Power'] / (df['DC_Power'] + 1e-6)
        df['Power_Per_Irradiance'] = df['AC_Power'] / (df['Irradiance'] + 1e-6)

        # === STRESS THERMIQUE ===
        logger.info("üå°Ô∏è Calcul des indicateurs thermiques...")
        df['Temp_Difference'] = df['Module_Temperature'] - df['Ambient_Temperature']
        df['Thermal_Stress'] = df['Module_Temperature'] * df['Irradiance'] / 1000
        df['Temperature_Efficiency_Loss'] = 0.005 * (df['Module_Temperature'] - 25)

        # === INDICATEURS DE RENDEMENT ===
        logger.info("[DATA] Calcul des indicateurs de rendement...")
        df['Power_Density'] = df['AC_Power'] / (df['DC_Voltage'] * df['DC_Current'] + 1e-6)
        df['Voltage_Current_Product'] = df['DC_Voltage'] * df['DC_Current']

        # === FEATURES DYNAMIQUES ===
        logger.info("üìà Cr√©ation des features dynamiques...")
        for col in ['AC_Power', 'DC_Power', 'Irradiance', 'Module_Temperature']:
            df[f'{col}_Rolling_Mean_1h'] = df.groupby('Inverter_ID')[col].transform(
                lambda x: x.rolling(window=4, min_periods=1).mean()
            )
            df[f'{col}_Rolling_Std_1h'] = df.groupby('Inverter_ID')[col].transform(
                lambda x: x.rolling(window=4, min_periods=1).std()
            )

        # === INDICATEURS JOUR/NUIT ===
        df['Is_Daytime'] = ((df['Hour'] >= 6) & (df['Hour'] <= 18)).astype(int)
        df['Is_Peak_Hour'] = ((df['Hour'] >= 10) & (df['Hour'] <= 14)).astype(int)

        # Remplissage des NaN
        df = df.fillna(0)

        self.df_processed = df

        logger.info(f"[OK] Feature Engineering termin√© : {len(df.columns)} features cr√©√©es")
        logger.info(f"üìã Nouvelles features : {[col for col in df.columns if col not in self.df_raw.columns]}")

        return self.df_processed

    def train_models(self, target_column: str = 'AC_Power') -> Dict:
        """
        Entra√Ænement des mod√®les IA

        Args:
            target_column: Variable cible √† pr√©dire

        Returns:
            Dictionnaire des m√©triques
        """
        logger.info(f"[AI] D√©marrage de l'entra√Ænement des mod√®les IA...")
        logger.info(f"üéØ Variable cible : {target_column}")

        # Pr√©paration des donn√©es
        df = self.df_processed.copy()

        # S√©lection des features
        exclude_cols = ['Timestamp', 'Inverter_ID', target_column]
        feature_cols = [col for col in df.columns if col not in exclude_cols]

        X = df[feature_cols]
        y = df[target_column]

        logger.info(f"[DATA] Features utilis√©es : {len(feature_cols)}")
        logger.info(f"üìà Taille du dataset : {len(X)} √©chantillons")

        # Split train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        logger.info(f"‚úÇÔ∏è Train: {len(X_train)}, Test: {len(X_test)}")

        # Normalisation
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        # === RANDOM FOREST ===
        logger.info("üå≤ Entra√Ænement Random Forest...")
        rf_model = RandomForestRegressor(
            n_estimators=100,
            max_depth=20,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        rf_model.fit(X_train_scaled, y_train)
        rf_pred = rf_model.predict(X_test_scaled)

        rf_metrics = {
            'MAE': mean_absolute_error(y_test, rf_pred),
            'RMSE': np.sqrt(mean_squared_error(y_test, rf_pred)),
            'R2': r2_score(y_test, rf_pred)
        }

        self.models['RandomForest'] = rf_model
        self.metrics['RandomForest'] = rf_metrics
        self.predictions['RandomForest'] = rf_pred

        logger.info(f"[OK] Random Forest - MAE: {rf_metrics['MAE']:.2f}, R¬≤: {rf_metrics['R2']:.4f}")

        # === GRADIENT BOOSTING ===
        logger.info("üöÄ Entra√Ænement Gradient Boosting...")
        gb_model = GradientBoostingRegressor(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=5,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42
        )
        gb_model.fit(X_train_scaled, y_train)
        gb_pred = gb_model.predict(X_test_scaled)

        gb_metrics = {
            'MAE': mean_absolute_error(y_test, gb_pred),
            'RMSE': np.sqrt(mean_squared_error(y_test, gb_pred)),
            'R2': r2_score(y_test, gb_pred)
        }

        self.models['GradientBoosting'] = gb_model
        self.metrics['GradientBoosting'] = gb_metrics
        self.predictions['GradientBoosting'] = gb_pred

        logger.info(f"[OK] Gradient Boosting - MAE: {gb_metrics['MAE']:.2f}, R¬≤: {gb_metrics['R2']:.4f}")

        # === ENSEMBLE MODEL ===
        logger.info("üé≠ Cr√©ation du mod√®le Ensemble...")
        ensemble_pred = (rf_pred + gb_pred) / 2

        ensemble_metrics = {
            'MAE': mean_absolute_error(y_test, ensemble_pred),
            'RMSE': np.sqrt(mean_squared_error(y_test, ensemble_pred)),
            'R2': r2_score(y_test, ensemble_pred)
        }

        self.metrics['Ensemble'] = ensemble_metrics
        self.predictions['Ensemble'] = ensemble_pred

        logger.info(f"[OK] Ensemble - MAE: {ensemble_metrics['MAE']:.2f}, R¬≤: {ensemble_metrics['R2']:.4f}")

        # S√©lection du meilleur mod√®le
        best_model_name = max(self.metrics, key=lambda x: self.metrics[x]['R2'])
        logger.info(f"üèÜ Meilleur mod√®le : {best_model_name} (R¬≤ = {self.metrics[best_model_name]['R2']:.4f})")

        # Importance des features (Random Forest)
        feature_importance = pd.DataFrame({
            'Feature': feature_cols,
            'Importance': rf_model.feature_importances_
        }).sort_values('Importance', ascending=False)

        self.feature_importance = feature_importance

        logger.info("\n[DATA] Top 10 Features importantes :")
        logger.info(feature_importance.head(10).to_string(index=False))

        # Sauvegarde des mod√®les
        self.save_models()

        # Stockage pour utilisation ult√©rieure
        self.df_train = pd.DataFrame(X_train_scaled, columns=feature_cols)
        self.df_train['y_true'] = y_train.values

        self.df_test = pd.DataFrame(X_test_scaled, columns=feature_cols)
        self.df_test['y_true'] = y_test.values
        self.df_test['y_pred_rf'] = rf_pred
        self.df_test['y_pred_gb'] = gb_pred
        self.df_test['y_pred_ensemble'] = ensemble_pred

        return self.metrics

    def detect_anomalies(self) -> pd.DataFrame:
        """
        D√©tection intelligente des anomalies avec Isolation Forest

        Returns:
            DataFrame avec anomalies d√©tect√©es
        """
        logger.info("üîç D√©marrage de la d√©tection des anomalies...")

        df = self.df_processed.copy()

        # S√©lection des features pour la d√©tection
        anomaly_features = [
            'AC_Power', 'DC_Power', 'Conversion_Efficiency',
            'Module_Temperature', 'Irradiance', 'DC_AC_Ratio',
            'Temp_Difference', 'Thermal_Stress'
        ]

        X_anomaly = df[anomaly_features].fillna(0)

        # Entra√Ænement Isolation Forest
        self.anomaly_detector = IsolationForest(
            contamination=0.05,  # 5% d'anomalies attendues
            random_state=42,
            n_jobs=-1
        )

        anomaly_labels = self.anomaly_detector.fit_predict(X_anomaly)
        anomaly_scores = self.anomaly_detector.score_samples(X_anomaly)

        df['Is_Anomaly'] = (anomaly_labels == -1).astype(int)
        df['Anomaly_Score'] = anomaly_scores

        # Classification par gravit√©
        def classify_severity(row):
            if row['Is_Anomaly'] == 0:
                return 'Normal'

            score = abs(row['Anomaly_Score'])
            efficiency = row['Conversion_Efficiency']
            temp_diff = row['Temp_Difference']

            # Crit√®res de gravit√©
            if efficiency < 0.7 or temp_diff > 40 or score > 0.7:
                return '√âlev√©e'
            elif efficiency < 0.85 or temp_diff > 30 or score > 0.5:
                return 'Moyenne'
            else:
                return 'Faible'

        df['Severity'] = df.apply(classify_severity, axis=1)

        # Type d'anomalie
        def classify_anomaly_type(row):
            if row['Is_Anomaly'] == 0:
                return 'Aucune'

            if row['Conversion_Efficiency'] < 0.8:
                return 'D√©faut de conversion DC/AC'
            elif row['Module_Temperature'] > 80:
                return 'Surchauffe module'
            elif row['DC_AC_Ratio'] > 1.3:
                return 'Dysfonctionnement onduleur'
            else:
                return 'Anomalie g√©n√©rale'

        df['Anomaly_Type'] = df.apply(classify_anomaly_type, axis=1)

        # Estimation des pertes √©nerg√©tiques
        normal_power = df[df['Is_Anomaly'] == 0]['AC_Power'].mean()
        df['Energy_Loss_kWh'] = df.apply(
            lambda row: max(0, (normal_power - row['AC_Power']) / 4) if row['Is_Anomaly'] == 1 else 0,
            axis=1
        )

        self.anomalies = df[df['Is_Anomaly'] == 1].copy()

        # Statistiques
        total_anomalies = df['Is_Anomaly'].sum()
        severity_counts = df['Severity'].value_counts()
        total_losses = df['Energy_Loss_kWh'].sum()

        logger.info(f"[OK] D√©tection termin√©e : {total_anomalies} anomalies d√©tect√©es ({total_anomalies/len(df)*100:.2f}%)")
        logger.info(f"\n[DATA] R√©partition par gravit√© :\n{severity_counts}")
        logger.info(f"‚ö° Pertes √©nerg√©tiques estim√©es : {total_losses:.2f} kWh")

        # Sauvegarde
        anomaly_report = self.anomalies.copy()
        output_path = self.outputs_dir / "anomalies_report.csv"
        anomaly_report.to_csv(output_path, index=False)
        logger.info(f"üíæ Rapport d'anomalies sauvegard√© : {output_path}")

        return df

    def generate_alerts(self, threshold_severity: str = 'Moyenne') -> pd.DataFrame:
        """
        G√©n√©ration d'alertes intelligentes avec recommandations

        Args:
            threshold_severity: Niveau de gravit√© minimum pour les alertes

        Returns:
            DataFrame des alertes
        """
        logger.info(f"üö® G√©n√©ration des alertes (gravit√© >= {threshold_severity})...")

        if self.anomalies is None or len(self.anomalies) == 0:
            logger.warning("‚ö†Ô∏è Aucune anomalie d√©tect√©e")
            return pd.DataFrame()

        severity_order = {'Faible': 1, 'Moyenne': 2, '√âlev√©e': 3}
        min_severity = severity_order.get(threshold_severity, 2)

        alerts = self.anomalies[
            self.anomalies['Severity'].map(severity_order) >= min_severity
        ].copy()

        # Recommandations techniques
        def get_recommendation(row):
            anomaly_type = row['Anomaly_Type']
            severity = row['Severity']

            recommendations = {
                'D√©faut de conversion DC/AC': f"URGENT - V√©rifier l'onduleur {row['Inverter_ID']} - Efficacit√© critique ({row['Conversion_Efficiency']:.2%})",
                'Surchauffe module': f"ATTENTION - Temp√©rature excessive ({row['Module_Temperature']:.1f}¬∞C) - V√©rifier la ventilation",
                'Dysfonctionnement onduleur': f"INTERVENTION - Ratio DC/AC anormal ({row['DC_AC_Ratio']:.2f}) - Diagnostic requis",
                'Anomalie g√©n√©rale': "Inspection recommand√©e - Comportement inhabituel d√©tect√©"
            }

            return recommendations.get(anomaly_type, "Inspection recommand√©e")

        alerts['Recommendation'] = alerts.apply(get_recommendation, axis=1)

        # Priorisation
        alerts['Priority'] = alerts['Severity'].map({
            'Faible': 3,
            'Moyenne': 2,
            '√âlev√©e': 1
        })

        alerts = alerts.sort_values('Priority')

        logger.info(f"[OK] {len(alerts)} alertes g√©n√©r√©es")
        logger.info(f"\nüö® Alertes prioritaires (Top 5) :")
        top_alerts = alerts.head(5)[['Timestamp', 'Inverter_ID', 'Severity', 'Anomaly_Type', 'Recommendation']]
        logger.info(top_alerts.to_string(index=False))

        # Sauvegarde
        output_path = self.outputs_dir / "alerts_active.csv"
        alerts.to_csv(output_path, index=False)
        logger.info(f"üíæ Alertes sauvegard√©es : {output_path}")

        return alerts

    def calculate_kpis(self) -> Dict:
        """
        Calcul des KPI industriels

        Returns:
            Dictionnaire des KPI
        """
        logger.info("[DATA] Calcul des KPI industriels...")

        df = self.df_processed

        # V√©rifier si les colonnes d'anomalies existent
        has_anomalies = 'Is_Anomaly' in df.columns and 'Energy_Loss_kWh' in df.columns

        kpis = {
            # Production
            'total_production_mwh': df['AC_Power'].sum() / 4000,  # 15min intervals -> MWh
            'avg_power_kw': df['AC_Power'].mean(),
            'max_power_kw': df['AC_Power'].max(),

            # Performance
            'capacity_factor': (df['AC_Power'].mean() / df['AC_Power'].max()) * 100,
            'avg_efficiency': df['Conversion_Efficiency'].mean() * 100 if 'Conversion_Efficiency' in df.columns else 0,
            'energy_yield': df['AC_Power'].sum() / (df['Irradiance'].sum() + 1e-6),

            # Disponibilit√©
            'availability': (1 - df['Is_Anomaly'].mean()) * 100 if has_anomalies else 95.0,
            'uptime_hours': len(df[df['AC_Power'] > 0]) / 4,

            # Pertes
            'total_losses_kwh': df['Energy_Loss_kWh'].sum() if has_anomalies else 0,
            'anomaly_rate': df['Is_Anomaly'].mean() * 100 if has_anomalies else 0,

            # Onduleurs
            'n_inverters': df['Inverter_ID'].nunique(),
            'best_inverter': df.groupby('Inverter_ID')['AC_Power'].mean().idxmax(),
            'worst_inverter': df.groupby('Inverter_ID')['AC_Power'].mean().idxmin(),
        }

        logger.info("\nüìà KPI Principaux :")
        logger.info(f"  Production totale : {kpis['total_production_mwh']:.2f} MWh")
        logger.info(f"  Facteur de capacit√© : {kpis['capacity_factor']:.1f}%")
        logger.info(f"  Rendement moyen : {kpis['avg_efficiency']:.1f}%")
        logger.info(f"  Disponibilit√© : {kpis['availability']:.1f}%")
        logger.info(f"  Pertes √©nerg√©tiques : {kpis['total_losses_kwh']:.2f} kWh")
        logger.info(f"  Meilleur onduleur : {kpis['best_inverter']}")

        # Sauvegarde
        kpi_df = pd.DataFrame([kpis])
        output_path = self.outputs_dir / "kpi_report.csv"
        kpi_df.to_csv(output_path, index=False)
        logger.info(f"üíæ Rapport KPI sauvegard√© : {output_path}")

        return kpis

    def save_models(self):
        """
        Sauvegarde des mod√®les entra√Æn√©s
        """
        logger.info("üíæ Sauvegarde des mod√®les...")

        # Sauvegarde du meilleur mod√®le (Random Forest)
        model_path = self.models_dir / "solar_predict_model.pkl"
        with open(model_path, 'wb') as f:
            pickle.dump({
                'model': self.models.get('RandomForest'),
                'scaler': self.scaler,
                'feature_importance': self.feature_importance,
                'metrics': self.metrics
            }, f)

        logger.info(f"[OK] Mod√®le principal sauvegard√© : {model_path}")

        # Sauvegarde de tous les mod√®les
        all_models_path = self.models_dir / "all_models.pkl"
        with open(all_models_path, 'wb') as f:
            pickle.dump({
                'models': self.models,
                'scaler': self.scaler,
                'metrics': self.metrics,
                'anomaly_detector': self.anomaly_detector
            }, f)

        logger.info(f"[OK] Tous les mod√®les sauvegard√©s : {all_models_path}")

    def load_models(self):
        """
        Chargement des mod√®les sauvegard√©s
        """
        model_path = self.models_dir / "all_models.pkl"

        if not model_path.exists():
            logger.warning("‚ö†Ô∏è Aucun mod√®le sauvegard√© trouv√©")
            return

        logger.info(f"üìÇ Chargement des mod√®les depuis : {model_path}")

        with open(model_path, 'rb') as f:
            saved_data = pickle.load(f)

        self.models = saved_data.get('models', {})
        self.scaler = saved_data.get('scaler')
        self.metrics = saved_data.get('metrics', {})
        self.anomaly_detector = saved_data.get('anomaly_detector')

        logger.info("[OK] Mod√®les charg√©s avec succ√®s")

    def generate_report(self) -> str:
        """
        G√©n√©ration d'un rapport complet

        Returns:
            Chemin du rapport g√©n√©r√©
        """
        logger.info("üìÑ G√©n√©ration du rapport complet...")

        report_lines = [
            "="*80,
            "SOLAR AI PLATFORM - RAPPORT D'ANALYSE COMPLET",
            "="*80,
            f"\nDate de g√©n√©ration : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"P√©riode d'analyse : {self.df_processed['Timestamp'].min()} ‚Üí {self.df_processed['Timestamp'].max()}",
            f"Nombre d'√©chantillons : {len(self.df_processed):,}",
            "\n" + "="*80,
            "\n[DATA] PERFORMANCE DES MOD√àLES IA",
            "-"*80,
        ]

        for model_name, metrics in self.metrics.items():
            report_lines.append(f"\n{model_name}:")
            report_lines.append(f"  MAE  : {metrics['MAE']:.2f} kW")
            report_lines.append(f"  RMSE : {metrics['RMSE']:.2f} kW")
            report_lines.append(f"  R¬≤   : {metrics['R2']:.4f}")

        report_lines.extend([
            "\n" + "="*80,
            "\nüîç D√âTECTION DES ANOMALIES",
            "-"*80,
        ])

        if self.anomalies is not None and len(self.anomalies) > 0:
            report_lines.append(f"Total anomalies d√©tect√©es : {len(self.anomalies):,}")
            report_lines.append(f"Taux d'anomalies : {len(self.anomalies)/len(self.df_processed)*100:.2f}%")

            # V√©rifier si la colonne Severity existe
            if 'Severity' in self.df_processed.columns:
                report_lines.append("\nR√©partition par gravit√© :")
                severity_counts = self.df_processed['Severity'].value_counts()
                for severity, count in severity_counts.items():
                    report_lines.append(f"  {severity}: {count:,} ({count/len(self.df_processed)*100:.2f}%)")
        else:
            report_lines.append("Aucune anomalie d√©tect√©e")

        report_lines.extend([
            "\n" + "="*80,
            "\nüìà TOP 10 FEATURES IMPORTANTES",
            "-"*80,
        ])

        if not self.feature_importance.empty:
            for idx, row in self.feature_importance.head(10).iterrows():
                report_lines.append(f"  {row['Feature']:40} : {row['Importance']:.4f}")

        report_lines.append("\n" + "="*80 + "\n")

        report_content = "\n".join(report_lines)

        # Sauvegarde
        report_path = self.outputs_dir / f"solar_ai_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)

        logger.info(f"[OK] Rapport g√©n√©r√© : {report_path}")
        print(report_content)

        return str(report_path)


def main():
    """
    Fonction principale d'ex√©cution
    """
    print("\n" + "="*80)
    print("üåû SOLAR AI PLATFORM - Plateforme Intelligente de Supervision")
    print("="*80 + "\n")

    # Initialisation
    platform = SolarAIPlatform()

    # G√©n√©ration de donn√©es de d√©monstration
    print("[DATA] G√©n√©ration de donn√©es simul√©es...")
    platform.generate_sample_data(n_inverters=5, days=30)

    # Feature Engineering
    print("\n[BUILD] Ing√©nierie des caract√©ristiques...")
    platform.feature_engineering()

    # Entra√Ænement des mod√®les
    print("\n[AI] Entra√Ænement des mod√®les IA...")
    platform.train_models()

    # D√©tection d'anomalies
    print("\nüîç D√©tection des anomalies...")
    platform.detect_anomalies()

    # G√©n√©ration d'alertes
    print("\nüö® G√©n√©ration des alertes...")
    platform.generate_alerts(threshold_severity='Moyenne')

    # Calcul des KPI
    print("\n[DATA] Calcul des KPI...")
    platform.calculate_kpis()

    # Rapport final
    print("\nüìÑ G√©n√©ration du rapport...")
    platform.generate_report()

    print("\n" + "="*80)
    print("[OK] PROCESSUS TERMIN√â AVEC SUCC√àS")
    print("="*80)
    print(f"\nüìÅ Les r√©sultats sont disponibles dans : {platform.outputs_dir}")
    print(f"üíæ Les mod√®les sont sauvegard√©s dans : {platform.models_dir}")
    print("\nüöÄ Lancez le dashboard avec : streamlit run app.py")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()